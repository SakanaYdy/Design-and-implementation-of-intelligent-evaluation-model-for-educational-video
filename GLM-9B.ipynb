{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:41:03.913876600Z",
     "start_time": "2024-12-03T15:41:03.199838600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<|endoftext|>'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 11\u001B[0m\n\u001B[0;32m      7\u001B[0m MODEL_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE:/DL/ZhipuAI/glm-4-9b-chat\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      9\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 11\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMODEL_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m你好\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     15\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mapply_chat_template([{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: query}],\n\u001B[0;32m     16\u001B[0m                                        add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     17\u001B[0m                                        tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     18\u001B[0m                                        return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     19\u001B[0m                                        return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     20\u001B[0m                                        )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:689\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    687\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(pretrained_model_name_or_path):\n\u001B[0;32m    688\u001B[0m         tokenizer_class\u001B[38;5;241m.\u001B[39mregister_for_auto_class()\n\u001B[1;32m--> 689\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m config_tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    691\u001B[0m     tokenizer_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1841\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   1838\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1839\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1841\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1843\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1844\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1845\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1846\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1847\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1849\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1850\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1851\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1852\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2077\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2074\u001B[0m         tokenizer\u001B[38;5;241m.\u001B[39madd_tokens(tokens, special_tokens\u001B[38;5;241m=\u001B[39mis_last_special)\n\u001B[0;32m   2076\u001B[0m \u001B[38;5;66;03m# Check all our special tokens are registered as \"no split\" token (we don't cut them) and are in the vocab\u001B[39;00m\n\u001B[1;32m-> 2077\u001B[0m added_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msanitize_special_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m added_tokens:\n\u001B[0;32m   2079\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning_advice(\n\u001B[0;32m   2080\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2081\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m fine-tuned or trained.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2082\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:856\u001B[0m, in \u001B[0;36mSpecialTokensMixin.sanitize_special_tokens\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    846\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msanitize_special_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    847\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    848\u001B[0m \u001B[38;5;124;03m    Make sure that all the special tokens attributes of the tokenizer (`tokenizer.mask_token`,\u001B[39;00m\n\u001B[0;32m    849\u001B[0m \u001B[38;5;124;03m    `tokenizer.cls_token`, etc.) are in the vocabulary.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;124;03m        `int`: The number of tokens added in the vocabulary during the operation.\u001B[39;00m\n\u001B[0;32m    855\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_special_tokens_extended\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspecial_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:999\u001B[0m, in \u001B[0;36mSpecialTokensMixin.add_tokens\u001B[1;34m(self, new_tokens, special_tokens)\u001B[0m\n\u001B[0;32m    996\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(new_tokens, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m    997\u001B[0m     new_tokens \u001B[38;5;241m=\u001B[39m [new_tokens]\n\u001B[1;32m--> 999\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspecial_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspecial_tokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils.py:421\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._add_tokens\u001B[1;34m(self, new_tokens, special_tokens)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m special_tokens \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdo_lower_case\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_lower_case:\n\u001B[0;32m    418\u001B[0m     token \u001B[38;5;241m=\u001B[39m token\u001B[38;5;241m.\u001B[39mlower()\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    420\u001B[0m     token \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munk_token\n\u001B[1;32m--> 421\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_tokens_to_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munk_token)\n\u001B[0;32m    422\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m token \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m tokens_to_add\n\u001B[0;32m    423\u001B[0m ):\n\u001B[0;32m    424\u001B[0m     tokens_to_add\u001B[38;5;241m.\u001B[39mappend(token)\n\u001B[0;32m    425\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils.py:575\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001B[1;34m(self, tokens)\u001B[0m\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    574\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tokens, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_token_to_id_with_added_voc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    577\u001B[0m ids \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    578\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tokens:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils.py:588\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001B[1;34m(self, token)\u001B[0m\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madded_tokens_encoder:\n\u001B[0;32m    587\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madded_tokens_encoder[token]\n\u001B[1;32m--> 588\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_token_to_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\glm-4-9b-chat\\tokenization_chatglm.py:89\u001B[0m, in \u001B[0;36mChatGLM4Tokenizer._convert_token_to_id\u001B[1;34m(self, token)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_convert_token_to_id\u001B[39m(\u001B[38;5;28mself\u001B[39m, token):\n\u001B[0;32m     88\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Converts a token (str) in an id using the vocab. \"\"\"\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmergeable_ranks\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mKeyError\u001B[0m: '<|endoftext|>'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # 设置 GPU 编号，如果单机单卡指定一个，单机多卡指定多个 GPU 编号\n",
    "# MODEL_PATH = \"THUDM/glm-4-9b-chat-hf\"\n",
    "MODEL_PATH = \"E:/DL/ZhipuAI/glm-4-9b-chat\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "query = \"你好\"\n",
    "\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": query}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       )\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: E:\\DL\\hub\\ZhipuAI/glm-4-9b-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 22:19:27,792 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading [config.json]:   0%|          | 0.00/1.40k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c76b8c4f40b47a9b28832ac4001085c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [configuration.json]:   0%|          | 0.00/36.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "889c6492e09c46ccb43caa42f656b816"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [configuration_chatglm.py]:   0%|          | 0.00/2.21k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96301716100b4027aa184e00a21bfdc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [generation_config.json]:   0%|          | 0.00/207 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c46d1ebfa35e4f04b5658af470f580b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [LICENSE]:   0%|          | 0.00/6.34k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b4cab064a7642b197c3c9be533c397a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00001-of-00010.safetensors]:   0%|          | 0.00/1.81G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "305c4f69421a4e85a80f62af597646e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00002-of-00010.safetensors]:   0%|          | 0.00/1.69G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de681c6863c947148e310eb4016ac0c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00003-of-00010.safetensors]:   0%|          | 0.00/1.83G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94c2237615d844428b2359ed5ca9468d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00004-of-00010.safetensors]:   0%|          | 0.00/1.80G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe8649826a8148a694ab9efc71fca66a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00005-of-00010.safetensors]:   0%|          | 0.00/1.69G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a44cd6a612204bc094efe11e5ebdfad0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00006-of-00010.safetensors]:   0%|          | 0.00/1.83G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aef06873a72349aeb66a2796b14c2c4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00007-of-00010.safetensors]:   0%|          | 0.00/1.80G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4fcce41443ad4b4a8451f389d221b692"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00008-of-00010.safetensors]:   0%|          | 0.00/1.69G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d353142a02a640a19c9bd655afd37625"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00009-of-00010.safetensors]:   0%|          | 0.00/1.83G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40b138ccc10b4e079b6ce2016799539d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model-00010-of-00010.safetensors]:   0%|          | 0.00/1.54G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d27531a71494cf4b1fbe78d28ae618e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [model.safetensors.index.json]:   0%|          | 0.00/28.4k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af9e1f0f50bf45b0aa270ed449653482"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [modeling_chatglm.py]:   0%|          | 0.00/46.2k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aec0472ea4814fceba73bf50d3e65167"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [README.md]:   0%|          | 0.00/8.70k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e63e085d67048518dcae998229f116f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [README_en.md]:   0%|          | 0.00/9.21k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73aca31254254d0aac3923dc56187402"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [tokenization_chatglm.py]:   0%|          | 0.00/8.78k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea8a91b8efb048c6995a75bf9f516240"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [tokenizer.model]:   0%|          | 0.00/2.50M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d0ee506bcfe474eb685d1ef00094a4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading [tokenizer_config.json]:   0%|          | 0.00/6.01k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7311e8e54404e2595ec532e63744f15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "model_dir = snapshot_download('ZhipuAI/glm-4-9b-chat', cache_dir='E:\\DL', revision='master')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:06:55.500325300Z",
     "start_time": "2024-12-03T14:19:24.532462500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<|endoftext|>'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m mode_name_or_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mE:/DL/ZhipuAI/glm-4-9b-chat\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# 从指定路径加载预训练的分词器\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# 从指定路径加载预训练的模型\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# torch_dtype指定数据类型，low_cpu_mem_usage优化CPU内存使用，trust_remote_code允许加载远程代码\u001B[39;00m\n\u001B[0;32m     16\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m     17\u001B[0m     mode_name_or_path,\n\u001B[0;32m     18\u001B[0m     torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbfloat16,\n\u001B[0;32m     19\u001B[0m     low_cpu_mem_usage\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     20\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     21\u001B[0m )\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# 将模型设置为评估模式\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:689\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    687\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(pretrained_model_name_or_path):\n\u001B[0;32m    688\u001B[0m         tokenizer_class\u001B[38;5;241m.\u001B[39mregister_for_auto_class()\n\u001B[1;32m--> 689\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m config_tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    691\u001B[0m     tokenizer_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1841\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   1838\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1839\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1841\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1843\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1844\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1845\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1846\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1847\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1849\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1850\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1851\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1852\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2077\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   2074\u001B[0m         tokenizer\u001B[38;5;241m.\u001B[39madd_tokens(tokens, special_tokens\u001B[38;5;241m=\u001B[39mis_last_special)\n\u001B[0;32m   2076\u001B[0m \u001B[38;5;66;03m# Check all our special tokens are registered as \"no split\" token (we don't cut them) and are in the vocab\u001B[39;00m\n\u001B[1;32m-> 2077\u001B[0m added_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msanitize_special_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2078\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m added_tokens:\n\u001B[0;32m   2079\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning_advice(\n\u001B[0;32m   2080\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2081\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m fine-tuned or trained.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2082\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:856\u001B[0m, in \u001B[0;36mSpecialTokensMixin.sanitize_special_tokens\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    846\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msanitize_special_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    847\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    848\u001B[0m \u001B[38;5;124;03m    Make sure that all the special tokens attributes of the tokenizer (`tokenizer.mask_token`,\u001B[39;00m\n\u001B[0;32m    849\u001B[0m \u001B[38;5;124;03m    `tokenizer.cls_token`, etc.) are in the vocabulary.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;124;03m        `int`: The number of tokens added in the vocabulary during the operation.\u001B[39;00m\n\u001B[0;32m    855\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall_special_tokens_extended\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspecial_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:999\u001B[0m, in \u001B[0;36mSpecialTokensMixin.add_tokens\u001B[1;34m(self, new_tokens, special_tokens)\u001B[0m\n\u001B[0;32m    996\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(new_tokens, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m    997\u001B[0m     new_tokens \u001B[38;5;241m=\u001B[39m [new_tokens]\n\u001B[1;32m--> 999\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspecial_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspecial_tokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils.py:421\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._add_tokens\u001B[1;34m(self, new_tokens, special_tokens)\u001B[0m\n\u001B[0;32m    417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m special_tokens \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdo_lower_case\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_lower_case:\n\u001B[0;32m    418\u001B[0m     token \u001B[38;5;241m=\u001B[39m token\u001B[38;5;241m.\u001B[39mlower()\n\u001B[0;32m    419\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    420\u001B[0m     token \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munk_token\n\u001B[1;32m--> 421\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_tokens_to_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_tokens_to_ids(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munk_token)\n\u001B[0;32m    422\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m token \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m tokens_to_add\n\u001B[0;32m    423\u001B[0m ):\n\u001B[0;32m    424\u001B[0m     tokens_to_add\u001B[38;5;241m.\u001B[39mappend(token)\n\u001B[0;32m    425\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils.py:575\u001B[0m, in \u001B[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001B[1;34m(self, tokens)\u001B[0m\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    574\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tokens, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 575\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_token_to_id_with_added_voc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    577\u001B[0m ids \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    578\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tokens:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils.py:588\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001B[1;34m(self, token)\u001B[0m\n\u001B[0;32m    586\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madded_tokens_encoder:\n\u001B[0;32m    587\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madded_tokens_encoder[token]\n\u001B[1;32m--> 588\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_token_to_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\glm-4-9b-chat\\tokenization_chatglm.py:89\u001B[0m, in \u001B[0;36mChatGLM4Tokenizer._convert_token_to_id\u001B[1;34m(self, token)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_convert_token_to_id\u001B[39m(\u001B[38;5;28mself\u001B[39m, token):\n\u001B[0;32m     88\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Converts a token (str) in an id using the vocab. \"\"\"\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmergeable_ranks\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[1;31mKeyError\u001B[0m: '<|endoftext|>'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 设定使用的设备为CUDA，如果CUDA不可用，将回退到CPU\n",
    "device = \"cuda\"\n",
    "\n",
    "# 指定预训练模型的路径\n",
    "mode_name_or_path = 'E:/DL/ZhipuAI/glm-4-9b-chat'\n",
    "\n",
    "# 从指定路径加载预训练的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)\n",
    "\n",
    "\n",
    "# 从指定路径加载预训练的模型\n",
    "# torch_dtype指定数据类型，low_cpu_mem_usage优化CPU内存使用，trust_remote_code允许加载远程代码\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mode_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ").to(device).eval()  # 将模型设置为评估模式\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-03T15:40:14.434446900Z",
     "start_time": "2024-12-03T15:40:14.002697400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义用户输入的查询文本\n",
    "query = \"请介绍一下AI大模型\"\n",
    "\n",
    "# 使用分词器的apply_chat_template方法来准备输入数据\n",
    "# 这个方法会根据聊天模板将用户输入格式化为模型可接受的格式\n",
    "# add_generation_prompt添加生成提示，tokenize进行分词，return_tensors指定返回PyTorch张量\n",
    "# return_dict指定返回字典格式，方便后续处理\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": query}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# 将输入数据移动到指定的设备上\n",
    "inputs = inputs.to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义生成文本时的参数\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 2500,  # 设置生成文本的最大长度\n",
    "    \"do_sample\": True,  # 是否从可能的下一个词中随机选择\n",
    "    \"top_k\": 1  # 从概率最高的k个词中选择\n",
    "}\n",
    "\n",
    "# 使用torch.no_grad()上下文管理器来禁用梯度计算，这在推理时可以减少内存使用\n",
    "with torch.no_grad():\n",
    "    # 使用模型的generate方法生成文本\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    # 截取生成的文本，去除开头的提示部分\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "\n",
    "    # 使用分词器的decode方法将生成的词ID解码回文本，并打印出来\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
